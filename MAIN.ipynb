{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import spearmanr as Spearman\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.corpus import words\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# for modeling\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import discriminant_analysis as da\n",
    "from sklearn import tree\n",
    "# from sklearn.cross_validation import cross_val_predict \n",
    "# from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from nltk.corpus import wordnet\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">append_regularised_score</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> dataframe \n",
    "- scores of each essay set are normalized by mean and standard deviation x = (x-mu)/std\n",
    "- creates new feature in dataframe \"std_score\"\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> new dataframe\n",
    "\n",
    "## <span style=\"color:blue\">create_regularization_data</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> dataframe \n",
    "- finds the mean and standard deviation of each essay\n",
    "- creates a list with essay number, mean of score, standard deviation of score\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> Regularised list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_regularized_scores(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    new_df['std_score'] = new_df.groupby(['essay_set'])[['score']].apply(lambda x: (x - np.mean(x)) / (np.std(x)))\n",
    "    return new_df\n",
    "\n",
    "def create_regularization_data(old_df):\n",
    "    #getting the number of datasets\n",
    "    max_essay_set = max(old_df['essay_set'])\n",
    "    #list of the regularized values\n",
    "    regularization_data = []\n",
    "    for i in range(max_essay_set+1):\n",
    "        mean = np.mean((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        std = np.std((old_df[old_df['essay_set'] == i + 1])['score'])\n",
    "        regularization_data.append([i + 1, mean, std])\n",
    "    return regularization_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Training Data\n",
    "- Scores of each essay becomes the aversage score of domain 1 and domain 2\n",
    "- create regularisation list\n",
    "- append regularised score to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularized data for each essay set =  [[1, 8.528323051037576, 1.5381336495587767], [2, 6.749444444444444, 1.3844371990179603], [3, 1.8482039397450754, 0.8149207612821795], [4, 1.4322033898305084, 0.9395167668768533], [5, 2.4088642659279778, 0.9705520523317599], [6, 2.72, 0.970360757656664], [7, 16.062460165710643, 4.583888354164165], [8, 36.95020746887967, 5.749521294509325], [9, nan, nan]]\n",
      "\n",
      "\n",
      "mean and standard deviation of essay set 1 =  5.145133400155731e-16 , 1.0000000000000064\n",
      "mean and standard deviation of essay set 2 =  1.8861455607242937e-16 , 1.0000000000000007\n",
      "mean and standard deviation of essay set 3 =  -8.542156296073047e-17 , 0.9999999999999976\n",
      "mean and standard deviation of essay set 4 =  -1.3303858956101453e-16 , 1.0000000000000004\n",
      "mean and standard deviation of essay set 5 =  1.1314433539046957e-16 , 0.999999999999986\n",
      "mean and standard deviation of essay set 6 =  -5.913787977836668e-16 , 0.9999999999999828\n",
      "mean and standard deviation of essay set 7 =  1.3200261647152516e-16 , 1.0000000000000007\n",
      "mean and standard deviation of essay set 8 =  -2.549059779913914e-17 , 1.0000000000000004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "      <th>std_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.343483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.306655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.993622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.956794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.343483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   score  std_score  \n",
       "0      8  -0.343483  \n",
       "1      9   0.306655  \n",
       "2      7  -0.993622  \n",
       "3     10   0.956794  \n",
       "4      8  -0.343483  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in training data\n",
    "# Note that for essay set 2, score becomes average of 2 domain scores\n",
    "train_cols = ['essay_id', 'essay_set', 'essay', 'domain1_score', 'domain2_score']\n",
    "train_df = pd.read_csv('training_set_rel3.tsv', delimiter='\\t', usecols=train_cols,dtype={'essay_set':int},encoding = \"ISO-8859-1\")\n",
    "for i in range(train_df.shape[0]):\n",
    "    if not np.isnan(train_df.get_value(i, 'domain2_score')):\n",
    "        assert (train_df.get_value(i, 'essay_set') == 2)\n",
    "        new_val = train_df.get_value(i, 'domain1_score') + train_df.get_value(i, 'domain2_score')\n",
    "        train_df.set_value(i, 'domain1_score', new_val) \n",
    "train_df = train_df.drop('domain2_score', axis=1)\n",
    "train_df = train_df.rename(columns={'domain1_score': 'score'})\n",
    "\n",
    "regularization_data = create_regularization_data(train_df)\n",
    "train_df = append_regularized_scores(train_df)\n",
    "\n",
    "print (\"The regularized data for each essay set = \", regularization_data)\n",
    "print (\"\\n\")\n",
    "\n",
    "#validate that the standardization works\n",
    "max_essay_set = max(train_df['essay_set'])\n",
    "for i in range (max_essay_set):\n",
    "    valid = train_df[train_df[\"essay_set\"] == i + 1][\"std_score\"]\n",
    "    print (\"mean and standard deviation of essay set \" + str(i + 1) + \" = \", np.mean(valid), \",\", np.std(valid))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show nothing is empty in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing training data!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if train_df.isnull().any().any():\n",
    "    print ('Training data is missing!')\n",
    "else:\n",
    "    print ('No missing training data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">vectorizer_clean</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> old dataframe \n",
    "- cleans essay and returns essay with only lowecase words separated by space\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> new dataframe with cleaned essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_clean(old_df):\n",
    "    new_df = old_df.copy()\n",
    "    for i in range(new_df.shape[0]):\n",
    "        new_df.set_value(i, 'essay', \" \".join(re.sub('[^a-zA-Z\\d\\s]', '', new_df['essay'].iloc[i]).lower().split())) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_id  essay_set                                              essay  \\\n",
      "0         1          1  dear local newspaper i think effects computers...   \n",
      "1         2          1  dear caps1 caps2 i believe that using computer...   \n",
      "2         3          1  dear caps1 caps2 caps3 more and more people us...   \n",
      "3         4          1  dear local newspaper caps1 i have found that m...   \n",
      "4         5          1  dear location1 i know having computers has a p...   \n",
      "\n",
      "   score  std_score  \n",
      "0      8  -0.343483  \n",
      "1      9   0.306655  \n",
      "2      7  -0.993622  \n",
      "3     10   0.956794  \n",
      "4      8  -0.343483  \n"
     ]
    }
   ],
   "source": [
    "# essay is now just lowercase words separated by space\n",
    "vectorizer_train = vectorizer_clean(train_df)\n",
    "print (vectorizer_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating 'y' for classification as well as regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_reg = vectorizer_train['std_score']\n",
    "train_std_scores = np.asarray(vectorizer_train['std_score'], dtype=\"byte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorizer\n",
    "- Create vectors from essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12976,)\n",
      "(12976, 43081)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12976"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = train_essays = vectorizer_train['essay'].values\n",
    "print(text.shape)\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(text)\n",
    "vector = vectorizer.transform(text)\n",
    "print(vector.shape)\n",
    "train_vectors1 = vector.toarray()\n",
    "len(train_std_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Unique words\n",
    "## <span style=\"color:blue\">fill_unique_words_column</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> dataframe \n",
    "- counts the number of unique words \n",
    "- Calculates ((no. of unique words) / (total words))\n",
    "- returns a list with percentages\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> list of percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def fill_unique_words_column(train_dff):\n",
    "\n",
    "    #percentage of unique words to the total number of words\n",
    "    unique_word_percentages_train = []\n",
    "\n",
    "    for i in range(len(train_df)):\n",
    "        splits = train_df.iloc[i][\"essay\"].split()\n",
    "        total_words = len(splits)\n",
    "        unique_words = len(Counter(splits))\n",
    "        percentage = float(unique_words) / total_words\n",
    "        unique_word_percentages_train.append(percentage)\n",
    "\n",
    "    return unique_word_percentages_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique = fill_unique_words_column(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Misspelled words\n",
    "\n",
    "## <span style=\"color:blue\">percentage_correct_spelling</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> a single essay text \n",
    "- checks if each word in the essay is a valid word or not using the wordnet database\n",
    "- generates percentage of correctly spelled words\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input is list of words in text, output percentage spelling correct\n",
    "def percentage_correct_spelling(text):\n",
    "    text_len = len(text)\n",
    "    correct = 0\n",
    "    for word in text:\n",
    "        try:\n",
    "            if wordnet.synsets(word):\n",
    "                correct += 1\n",
    "        except:\n",
    "            correct+= 0\n",
    "    return 1. * correct / text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.685459940652819]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_feature_x = []\n",
    "for train in train_essays:\n",
    "    sentence = train.split()\n",
    "    percent = percentage_correct_spelling(sentence)\n",
    "    spelling_feature_x.append([percent])\n",
    "spelling_feature_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Sentences\n",
    "## <span style=\"color:blue\">sentences</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> essay text\n",
    "- generates the number of sentences in the essay\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> length of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences(par):\n",
    "    split_sent = re.split(r'[.!?]+', par)\n",
    "    return len(split_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numOfSent_train = []\n",
    "for essay in train_df['essay']:\n",
    "    sent = sentences(essay)\n",
    "    numOfSent_train.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12976"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(numOfSent_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate POS Tags\n",
    "## <span style=\"color:blue\">create_tags_dict</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> essay\n",
    "- calculates proportion of each part of speech in essay\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> dict(tag:proportion)\n",
    "\n",
    "## <span style=\"color:blue\">fill_pos_columns</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> datxaframe\n",
    "- calculates proportion of each part of speech in each essay of the dataframe\n",
    "- appends it to the repective column in the dataframe\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> new dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNIV_TAGS = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
    "\n",
    "def create_tags_dict(essay):\n",
    "    text = word_tokenize(essay)\n",
    "    num_tokens = len(text)\n",
    "    tagged_words = nltk.pos_tag(text, tagset='universal')\n",
    "    tags_only = [tag for _, tag in tagged_words]\n",
    "    fd = FreqDist(tags_only)\n",
    "    tags_dict = {}\n",
    "    for pos in UNIV_TAGS:\n",
    "        tags_dict[pos] = float(fd[pos]) / num_tokens\n",
    "\n",
    "    return tags_dict\n",
    "\n",
    "def fill_pos_columns(df):\n",
    "    for pos in UNIV_TAGS:\n",
    "        df[pos] = pd.Series([0.0] * df.shape[0], index=df.index)\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        essay = df.get_value(i, 'essay')\n",
    "        tags = create_tags_dict(essay)\n",
    "        #print (tags)\n",
    "        for pos in UNIV_TAGS:\n",
    "            df = df.set_value(i, pos, tags[pos])\n",
    "    \n",
    "    return df['ADJ'],df['ADP'],df['ADV'],df['CONJ'],df['DET'],df['NOUN'],df['NUM'],df['PRT'],df['PRON'],df['VERB'],df['.'],df['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12976,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lADJ, lADP, lADV, lCONJ, lDET, lNOUN, lNUM, lPRT, lPRON, lVERB, lfullstop, lX= \\\n",
    "fill_pos_columns(train_df)\n",
    "lADP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Perplexity\n",
    "## <span style=\"color:blue\">perplexity_clean</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> df\n",
    "- cleans the essays\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> list of strings\n",
    "## <span style=\"color:blue\">Perplexity</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> dataframe\n",
    "- class that helps calculate perplexities of each essay\n",
    "- appends all the the perplexities into a list which is later used as a feature\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> list of perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perplexity\n",
    "def perplexity_clean(df):\n",
    "    essays_string = \"\"\n",
    "    for i in range(df.shape[0]):\n",
    "    \tessay = df.get_value(i, 'essay')\n",
    "    \tessays_string += (\" \".join(re.sub('[^a-zA-Z\\d\\s]', '', essay).lower().split()))\n",
    "    return [essays_string]\n",
    "\n",
    "class Perplexity:\n",
    "    def __init__(self):\n",
    "        self.num_words = None\n",
    "        self.counts = None\n",
    "        self.vectorizer = None\n",
    "\n",
    "    def create_counts(self, compressed_essays):\n",
    "        self.vectorizer = CountVectorizer().fit(compressed_essays)\n",
    "        self.counts = self.vectorizer.transform(compressed_essays).toarray()[0]\n",
    "\n",
    "        # length added for LaPlace smoothing\n",
    "        self.num_words = float(sum(self.counts) + len(self.counts))\n",
    "\n",
    "    def fill_perplexity_columns(self, train_df):\n",
    "        print(\"Creating ngram counts...\")\n",
    "        self.create_counts(perplexity_clean(train_df))\n",
    "        train_clean = vectorizer_clean(train_df)\n",
    "        for i in range(train_clean.shape[0]):\n",
    "            essay = train_df.get_value(i, 'essay')\n",
    "            perp = self.perplexity(essay)\n",
    "            train_df = train_df.set_value(i, 'perplexity', perp)\n",
    "        return train_df['perplexity']\n",
    "\n",
    "    # After having already fit model on a set of training essays, calculates the\n",
    "    # perplexity of a student's essay based from the model, and returns this\n",
    "    # perplexity to be used as a feature\n",
    "    def perplexity(self, test_essay):\n",
    "        log_prob = 0.0\n",
    "        word_list = test_essay.split()\n",
    "        for word in word_list:\n",
    "            if word in self.vectorizer.vocabulary_:\n",
    "                log_prob += math.log( (self.counts[self.vectorizer.vocabulary_[word]] + 1.0) / self.num_words)\n",
    "            else:\n",
    "                log_prob += math.log (1.0 / self.num_words)\n",
    "\n",
    "        return math.pow(2.0, -log_prob / len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ngram counts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         273.964130\n",
       "1         297.652243\n",
       "2         236.759940\n",
       "3         354.237007\n",
       "4         237.657729\n",
       "5         226.584627\n",
       "6         284.796399\n",
       "7         316.965611\n",
       "8         379.586216\n",
       "9         217.551144\n",
       "10        459.728385\n",
       "11        287.106295\n",
       "12        202.291153\n",
       "13        294.971815\n",
       "14        250.868614\n",
       "15        323.696663\n",
       "16        240.958775\n",
       "17        265.156884\n",
       "18       1905.298557\n",
       "19        382.859252\n",
       "20        211.703701\n",
       "21        130.271133\n",
       "22        240.800801\n",
       "23        396.751435\n",
       "24        201.791930\n",
       "25        274.297495\n",
       "26        261.197957\n",
       "27        330.107505\n",
       "28        335.627776\n",
       "29        245.131859\n",
       "            ...     \n",
       "12946     454.539369\n",
       "12947     301.221928\n",
       "12948     308.559467\n",
       "12949     229.827426\n",
       "12950     326.853295\n",
       "12951     425.548438\n",
       "12952     413.852151\n",
       "12953     431.997842\n",
       "12954     376.876596\n",
       "12955     288.586590\n",
       "12956     301.297719\n",
       "12957     386.473196\n",
       "12958     199.893427\n",
       "12959     461.434296\n",
       "12960     328.436666\n",
       "12961     433.683195\n",
       "12962     383.369245\n",
       "12963     277.934733\n",
       "12964     325.141787\n",
       "12965     320.731091\n",
       "12966     265.082869\n",
       "12967     208.837748\n",
       "12968     391.382110\n",
       "12969     211.737510\n",
       "12970     176.504939\n",
       "12971     319.363710\n",
       "12972     322.138327\n",
       "12973     484.947292\n",
       "12974     519.612135\n",
       "12975     438.641047\n",
       "Name: perplexity, Length: 12976, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perp = Perplexity().fill_perplexity_columns(train_df)\n",
    "perp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test train dataset\n",
    "## <span style=\"color:blue\">separate</span>\n",
    "#### <span style=\"color:green\">INPUT:</span> Complete dataset (vectors + features)\n",
    "- separates the vectors and features\n",
    "#### <span style=\"color:purple\">OUTPUT:</span> (vectors, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell, sent = spelling_feature_x, numOfSent_train\n",
    "y = train_std_scores\n",
    "X = []\n",
    "t = train_vectors1\n",
    "#print(type(t))\n",
    "for i,j in zip(range(len(spell)),t):\n",
    "    #print(j)\n",
    "    X.append([j,perp[i],spell[i][0],sent[i],unique[i],lADJ[i], lADP[i], lADV[i], lCONJ[i], lDET[i], lNOUN[i], lNUM[i], lPRT[i], lPRON[i], lVERB[i], lfullstop[i], lX[i]])\n",
    "#print(X[0])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9732x43081 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1052778 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def separate(X):\n",
    "    vector=[]\n",
    "    features=[]\n",
    "    for i in X:\n",
    "        vector.append(i[0])\n",
    "        features.append([i[1],i[2]])\n",
    "    return sparse.csr_matrix(vector),features\n",
    "\n",
    "vec_train,fea_train = separate(X_train)\n",
    "vec_test, fea_test = separate(X_test)\n",
    "vec_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7043773119605425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6707768187422934\n"
     ]
    }
   ],
   "source": [
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear')\n",
    "logistic_l2.fit(vec_train, y_train)\n",
    "pred2a = logistic_l2.predict(vec_test)\n",
    "print(accuracy_score(pred2a, y_test))\n",
    "\n",
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear')\n",
    "logistic_l2.fit(fea_train, y_train)\n",
    "pred2b = logistic_l2.predict(fea_test)\n",
    "print(accuracy_score(pred2b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7065351418002466\n",
      "0.6707768187422934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear')\n",
    "logistic_l1.fit(vec_train, y_train)\n",
    "pred1a = logistic_l1.predict(vec_test)\n",
    "print(accuracy_score(pred1a, y_test))\n",
    "\n",
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear')\n",
    "logistic_l1.fit(fea_train, y_train)\n",
    "pred1b = logistic_l1.predict(fea_test)\n",
    "print(accuracy_score(pred1b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2180, 1: 499, -1: 494, -2: 45, 2: 18, -3: 4, -4: 3, 4: 1})\n",
      "Counter({0: 2922, -1: 242, 1: 80})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(y_test))\n",
    "print(Counter(pred1a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the ensemble model\n",
    "estimator = []\n",
    "estimator.append(('logisticL1',logistic_l1))\n",
    "estimator.append(('logisticL2',logistic_l2))\n",
    "#estimator.append(('Ridge',ridge))\n",
    "#estimator.append(('Lasso',lasso))\n",
    "ensemble = VotingClassifier(estimator)\n",
    "v, f = separate(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6743980716433444\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "results = model_selection.cross_val_score(ensemble, f, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045229420031768\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "results = model_selection.cross_val_score(ensemble, v, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_std_scores\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_reg)\n",
    "vec_train,fea_train = separate(X_train)\n",
    "vec_test, fea_test = separate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ridge = linear_model.Ridge(alpha = 0.05)\n",
    "ridge.fit(vec_train, y_train)\n",
    "pred3a = ridge.predict(vec_test)\n",
    "#print(accuracy_score(pred3a, y_test))\n",
    "\n",
    "ridge = linear_model.Ridge(alpha = 0.05)\n",
    "ridge.fit(fea_train, y_train)\n",
    "pred3b = ridge.predict(fea_test)\n",
    "#print(accuracy_score(pred3b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: TFIDF 0.6518760968649936\n",
      "Ridge: FEATURES 0.10155101392527537\n"
     ]
    }
   ],
   "source": [
    "corr, p = Spearman(a = pred3a, b = y_test)\n",
    "print (\"Ridge: TFIDF\", corr)\n",
    "corr, p = Spearman(a = pred3b, b = y_test)\n",
    "print (\"Ridge: FEATURES\", corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha = 0.5)\n",
    "lasso.fit(vec_train, y_train)\n",
    "pred4a = lasso.predict(vec_test)\n",
    "#print(accuracy_score(pred3a, y_test))\n",
    "\n",
    "lasso = linear_model.Ridge(alpha = 0.05)\n",
    "lasso.fit(fea_train, y_train)\n",
    "pred4b = lasso.predict(fea_test)\n",
    "#print(accuracy_score(pred3b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: TFIDF nan\n",
      "Lasso: FEATURES 0.10155101392527537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "corr, p = Spearman(a = pred4a, b = y_test)\n",
    "print (\"Lasso: TFIDF\", corr)\n",
    "corr, p = Spearman(a = pred4b, b = y_test)\n",
    "print (\"Lasso: FEATURES\", corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13008353, -0.862997  , -0.99807548, ...,  0.45930567,\n",
       "        0.28549115,  0.78389842])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Combining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6388854352480297, pvalue=0.0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "def combine(true, pred):\n",
    "    array = []\n",
    "    for i in true:\n",
    "        val = stats.trim_mean(i,0.2)\n",
    "        array.append(val)\n",
    "    return Spearman(array,pred)\n",
    "\n",
    "def make(pred3a, pred3b, pred4a,pred4b):\n",
    "    pred_array=[]\n",
    "    for a,b,c,d in zip(pred3a,pred3b,pred4a,pred4b):\n",
    "        pred_array.append([a,b,c,d])\n",
    "    return pred_array\n",
    "pred_array=make(pred3a, pred3b, pred4a,pred4b)\n",
    "combine(pred_array,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Without POS tag and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[array([0., 0., 0., ..., 0., 0., 0.]), 0.685459940652819, 17, 0.5443786982248521]\n"
     ]
    }
   ],
   "source": [
    "spell, sent = spelling_feature_x, numOfSent_train\n",
    "y = train_std_scores\n",
    "X = []\n",
    "t = train_vectors1\n",
    "print(type(t))\n",
    "for i,j in zip(range(len(spell)),t):\n",
    "    #print(j)\n",
    "    X.append([j,spell[i][0],sent[i],unique[i]])\n",
    "print(X[0])\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1296: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6707768187422934\n",
      "0.6707768187422934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1296: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    }
   ],
   "source": [
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear', n_jobs=4)\n",
    "logistic_l2.fit(vec_train, y_train)\n",
    "pred2a = logistic_l2.predict(vec_test)\n",
    "print(accuracy_score(pred2a, y_test))\n",
    "\n",
    "logistic_l2 = LogReg(penalty='l2', solver='liblinear', n_jobs=4)\n",
    "logistic_l2.fit(fea_train, y_train)\n",
    "pred2b = logistic_l2.predict(fea_test)\n",
    "print(accuracy_score(pred2b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1296: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6707768187422934\n",
      "0.6707768187422934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1296: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    }
   ],
   "source": [
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear', n_jobs=4)\n",
    "logistic_l1.fit(vec_train, y_train)\n",
    "pred1a = logistic_l1.predict(vec_test)\n",
    "print(accuracy_score(pred1a, y_test))\n",
    "\n",
    "logistic_l1 = LogReg(penalty='l1', solver='liblinear', n_jobs=4)\n",
    "logistic_l1.fit(fea_train, y_train)\n",
    "pred1b = logistic_l1.predict(fea_test)\n",
    "print(accuracy_score(pred1b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7045229420031768\n"
     ]
    }
   ],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=7)\n",
    "results = model_selection.cross_val_score(ensemble, f, y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_std_scores\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_reg)\n",
    "vec_train,fea_train = separate(X_train)\n",
    "vec_test, fea_test = separate(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ridge = linear_model.Ridge(alpha = 0.05)\n",
    "ridge.fit(vec_train, y_train)\n",
    "pred3a = ridge.predict(vec_test)\n",
    "#print(accuracy_score(pred3a, y_test))\n",
    "\n",
    "ridge = linear_model.Ridge(alpha = 0.05)\n",
    "ridge.fit(fea_train, y_train)\n",
    "pred3b = ridge.predict(fea_test)\n",
    "#print(accuracy_score(pred3b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: TFIDF 0.6434234452517597\n",
      "Ridge: FEATURES 0.4047207740600403\n"
     ]
    }
   ],
   "source": [
    "corr, p = Spearman(a = pred3a, b = y_test)\n",
    "print (\"Ridge: TFIDF\", corr)\n",
    "corr, p = Spearman(a = pred3b, b = y_test)\n",
    "print (\"Ridge: FEATURES\", corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha = 0.5)\n",
    "lasso.fit(vec_train, y_train)\n",
    "pred4a = lasso.predict(vec_test)\n",
    "#print(accuracy_score(pred3a, y_test))\n",
    "\n",
    "lasso = linear_model.Ridge(alpha = 0.05)\n",
    "lasso.fit(fea_train, y_train)\n",
    "pred4b = lasso.predict(fea_test)\n",
    "#print(accuracy_score(pred3b, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso: TFIDF nan\n",
      "Lasso: FEATURES 0.4047207740600403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    }
   ],
   "source": [
    "corr, p = Spearman(a = pred4a, b = y_test)\n",
    "print (\"Lasso: TFIDF\", corr)\n",
    "corr, p = Spearman(a = pred4b, b = y_test)\n",
    "print (\"Lasso: FEATURES\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Combining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:2831: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.mean(atmp[sl], axis=axis)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6385863721747177, pvalue=0.0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "def combine(true, pred):\n",
    "    array = []\n",
    "    for i in true:\n",
    "        val = stats.trim_mean(i,0.2)\n",
    "        array.append(val)\n",
    "    return Spearman(array,pred)\n",
    "\n",
    "def make(pred3a, pred3b, pred4a,pred4b):\n",
    "    pred_array=[]\n",
    "    for a,b,c,d in zip(pred3a,pred3b,pred4a,pred4b):\n",
    "        pred_array.append([a,b,c,d])\n",
    "    return pred_array\n",
    "pred_array=make(pred3a, pred3b, pred4a,pred4b)\n",
    "combine(pred_array,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
